{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7ee184",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Kmeans cluster\n",
    "\n",
    "This part will do the Kmean cluster to get which color is most dominant in the picture\n",
    "Each cluster identified by K-Means becomes a filter. The region under the mask is available to future analysis.\n",
    "Here this includes gcc, rcc (and bcc) graphs over time for the original images as well as each of the filters.\n",
    "To do: statistical analysis of the filter outputs to decide if:\n",
    "1) Filters shoudl be merged because they behave like each other... therefore provide little new information individually.\n",
    "2) Add tick marks to the xcc graphs to indication the start, mean maximum and end of phenophase for the species at the NEON site. Those species lists are in the NEON site metadata file.\n",
    "3) The gcc, rcc shoudl be correlated with human phenology observation data to see if changes in rcc or gcc are aligned with phenophase human observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ff5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray, rgb2lab, deltaE_cie76\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import time\n",
    "from scipy import ndimage\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134f39d1-f960-4afe-a49a-f3ad5ac2c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Color identification - RGB to Hex Conversion\n",
    "def RGB2HEX255(color):\n",
    "    #return \"#{:02x}{:02x}{:02x}\".format(int(color[0]), int(color[1]), int(color[2]))    \n",
    "    return \"#{:02x}{:02x}{:02x}\".format(color[0], color[1], color[2])\n",
    "\n",
    "#Color identification - RGB to Hex Conversion\n",
    "def RGB2HEX(color):\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(int(color[0]*255), int(color[1]*255), int(color[2]*255))\n",
    "\n",
    "WINDOW_NAME = \"win\"\n",
    "def swap2images(image1, image2):\n",
    "    \"\"\" Display an image with cv2 replace with another image\n",
    "    for X seconds. can show contrast between \n",
    "    \"\"\"\n",
    "    cv2.namedWindow(WINDOW_NAME)\n",
    "    initialtime = time.time()\n",
    "\n",
    "    cv2.startWindowThread()\n",
    "\n",
    "    while (time.time() - initialtime < 8):\n",
    "        cv2.imshow(WINDOW_NAME, image1)\n",
    "        cv2.waitKey(500)\n",
    "        cv2.imshow(WINDOW_NAME, image2)\n",
    "        cv2.waitKey(500)\n",
    "    #print (\"before first waitkey(1)\")\n",
    "    cv2.waitKey(1)\n",
    "    cv2.destroyAllWindows()\n",
    "    #print (\"before second waitkey(1)\")\n",
    "    cv2.waitKey(1)\n",
    "    return(1)\n",
    "\n",
    "def ColorClusterPie(pie_labels, pie_center_colors):\n",
    "    \"\"\"\n",
    "    This function will perform the K mean and display it into to the pie chart\n",
    "    \n",
    "    params:\n",
    "        labels: the list of labels available in the image\n",
    "    \"\"\"\n",
    "    # Count the frequency of each label/color\n",
    "    label_counts = np.bincount(pie_labels)\n",
    "\n",
    "    # Create a list of (label, frequency) tuples\n",
    "    label_freq_pairs = [(label, count) for label, count in enumerate(label_counts)]\n",
    "\n",
    "    # Sort the list by frequency in descending order\n",
    "    label_freq_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Create a sorted ndarray of Center-colors based on label frequency\n",
    "    sorted_center_colors = np.array([pie_center_colors[label] for label, _ in label_freq_pairs])\n",
    "\n",
    "    # Extract the frequencies and labels from the sorted pairs\n",
    "    frequencies = [freq for _, freq in label_freq_pairs]\n",
    "    color_labels = [f'Color {i}: {sorted_center_colors[i]}' for i in range(len(label_freq_pairs))]\n",
    "\n",
    "    # Plot the pie chart\n",
    "    plt.pie(frequencies, labels=color_labels, colors=sorted_center_colors/255.0, autopct='%1.1f%%')\n",
    "    plt.axis('equal')  # Equal aspect ratio ensures circular pie\n",
    "    plt.show()\n",
    " \n",
    "    return sorted_center_colors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df66780-e129-481f-be48-07523c4e72aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given an RGB image, replace one of the colors with an RGB value to be replaced\n",
    "Plot the result\n",
    "\"\"\"\n",
    "def replaceAColor (image, target_rgb, replacement_rgb):\n",
    "    # the images that will have a color replaced\n",
    "    color_deleted_image = np.uint8(image)\n",
    "\n",
    "    # Find indices where target_rgb occurs in the image\n",
    "    indices = np.where(np.all(color_deleted_image == target_rgb, axis=-1))\n",
    "    #print('indicies.shape {0}',format(indices.shape))\n",
    "    # Replace the values at the indices with replacement_rgb\n",
    "    color_deleted_image[indices] = replacement_rgb\n",
    "    return color_deleted_image\n",
    "\n",
    "# not working!!! replace a list of different colors with a color\n",
    "def replaceMultipleColors (image, target_rgb_list, replacement_rgb):\n",
    "    color_deleted_image = np.uint8(image)\n",
    "    for target_rgbi in target_rgb_list:\n",
    "        #print(target_rgbi)\n",
    "        color_deleted_image = replaceAColor (color_deleted_image, target_rgbi, replacement_rgb)\n",
    "    return color_deleted_image\n",
    "\n",
    "def replaceAllButOneColor(image, unchanged_rgb, replacement_rgb):  \n",
    "     # the images that will have a color replaced\n",
    "    color_deleted_image = np.uint8(image)\n",
    "\n",
    "    # Find indices where target_rgb occurs in the image\n",
    "    indices = np.where(~np.all(color_deleted_image == unchanged_rgb, axis=-1))\n",
    "    #inverse_indices = np.where(~indices)[0]\n",
    "    # Replace the values at the indices with replacement_rgb\n",
    "    color_deleted_image[indices] = replacement_rgb\n",
    "    return color_deleted_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca0be1-5fcf-48a7-a72c-ac9d4d533e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_xcc (xcc, title, caption):\n",
    "    # Extract the magnitudes of rcc, gcc, and bcc from the first column of xcc\n",
    "    rcc = [data[0][0] for data in xcc]\n",
    "    gcc = [data[0][1] for data in xcc]\n",
    "    bcc = [data[0][2] for data in xcc]\n",
    "\n",
    "    # Create the x-axis values (row numbers)\n",
    "    x = range(len(xcc))\n",
    "\n",
    "    # Plot the lines\n",
    "    plt.plot(x, rcc, color='red', label='rcc')\n",
    "    plt.plot(x, gcc, color='green', label='gcc')\n",
    "    plt.plot(x, bcc, color='blue', label='bcc')\n",
    "\n",
    "    # Set the axis labels and title\n",
    "    plt.xlabel(caption)\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.title(title)\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    return 1\n",
    "\n",
    "####################################################################\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_rcc (rcc, StartFlower, EndFlower, StartLeafBud, EndLeafBud, title, caption):\n",
    "    # Extract the magnitudes of rcc, gcc, and bcc from the first column of xcc\n",
    "\n",
    "    # Create the x-axis values (row numbers)\n",
    "    x = range(len(rcc))\n",
    "\n",
    "    # Plot the lines\n",
    "    plt.plot(x, rcc, color='red', label='rcc')\n",
    "#    plt.plot(x, gcc, color='green', label='gcc')\n",
    "#    plt.plot(x, bcc, color='blue', label='bcc')\n",
    "    plt.plot([StartFlower, EndFlower], [rcc[StartFlower], rcc[StartFlower]], color='black', label='Flowering')\n",
    "    plt.plot([StartLeafBud, EndLeafBud], [rcc[StartLeafBud], rcc[StartLeafBud]], color='green', label='Leaf buds')\n",
    "\n",
    "    # Set the axis labels and title\n",
    "    plt.xlabel(caption)\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.title(title)\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    return 1\n",
    "\n",
    "####################################################################\n",
    "def slow_horizontal_variance(im):\n",
    "    '''Return average variance of horizontal lines of a grayscale image'''\n",
    "    #width, height = im.size\n",
    "    width, height = im.shape\n",
    "    if not width or not height: return 0\n",
    "    vars = []\n",
    "    #pix = im.load()\n",
    "    pix = im\n",
    "    for y in range(height):\n",
    "        row = [pix[x,y] for x in range(width)]\n",
    "        mean = sum(row)/width\n",
    "        variance = sum([(x-mean)**2 for x in row])/width\n",
    "        vars.append(variance)\n",
    "    return sum(vars)/height\n",
    "\n",
    "\"\"\"\n",
    "https://harvardforest1.fas.harvard.edu/sites/harvardforest.fas.harvard.edu/files/publications/pdfs/Sonnentag_AgriFarmMag_2011.pdf\n",
    "A widely used\n",
    "example to describe canopy greenness is excess green (ExG) defined\n",
    "as:\n",
    "2G − (R + B) (2)\n",
    "Excess green was found to be superior over other color indices for\n",
    "the distinction between green plants and soil/residue background\n",
    "by enhancing the signal from green plant material (Woebbecke\n",
    "et al., 1995).\n",
    "\"\"\"\n",
    "\n",
    "####################################################################\n",
    "import numpy as np\n",
    "\n",
    "def calculate_excess_green(img):\n",
    "    # Store the original data type\n",
    "    original_dtype = img.dtype\n",
    "    \n",
    "    # Convert image array to float64 data type\n",
    "    img = img.astype(np.float64)\n",
    "    \n",
    "    # Create an empty array to store the ExG values\n",
    "    exg_img = np.zeros_like(img, dtype=np.float64)\n",
    "    \n",
    "    # Iterate through each pixel\n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "            # Extract the red, green, and blue values for the current pixel\n",
    "            R = img[i, j, 0]\n",
    "            G = img[i, j, 1]\n",
    "            B = img[i, j, 2]\n",
    "            \n",
    "            # Calculate the excess green (ExG) using the formula: 2G - (R + B)\n",
    "            exg = (2 * G) - (R + B)\n",
    "            \n",
    "            # Update the green value of the pixel with the ExG value\n",
    "            exg_img[i, j, 1] = exg\n",
    "    \n",
    "    # Convert the image array back to the original data type\n",
    "    exg_img = exg_img.astype(original_dtype)\n",
    "    \n",
    "    return exg_img\n",
    "\n",
    "####################################################################\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def is_image_mostly_black(gray_image, threshold):\n",
    "    # Load the image\n",
    "    \n",
    "    # Calculate the percentage of black pixels\n",
    "    total_pixels = gray_image.size\n",
    "    black_pixels = np.sum(gray_image == 0)\n",
    "    black_ratio = black_pixels / total_pixels\n",
    "\n",
    "    # Check if the black ratio is above the threshold\n",
    "    if black_ratio >= threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# calculate gcc, rcc and bcc for the original image and for the masks of that image\n",
    "def get_xcc(image_names, \n",
    "            clustered_image, \n",
    "            labels, \n",
    "            ordered_center_color_index,\n",
    "            hazy_days):\n",
    "    xcc_over_time = [] # inner list is 0=R, 2=G, 3=B\n",
    "    DILATE = 1\n",
    "    NoDILATE = 0\n",
    "    i = 0\n",
    "    for image_file in image_names:\n",
    "        is_missing_day = False\n",
    "        isBlack = False\n",
    "        fog = False\n",
    "        if not image_file.endswith(\"jpg\"):\n",
    "            is_missing_day = True\n",
    "        else:\n",
    "            img = cv2.imread(image_file)\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        #swap2images(img, clustered_image)\n",
    "        if is_missing_day:\n",
    "            xcc_over_time.append([[ np.nan, np.nan, np.nan], fog, is_missing_day, -1])\n",
    "        elif is_image_mostly_black(gray, .9) == True or image_file in hazy_days:\n",
    "            fog = True\n",
    "            xcc_over_time.append([[ np.nan, np.nan, np.nan], fog, is_missing_day, -1])\n",
    "            isblack = True\n",
    "        else:\n",
    "            # var = slow_horizontal_variance(gray)\n",
    "            # fog = var > 3000    # FOG THRESHOLD\n",
    "            print (\"Processing: \", image_file, end='\\r')\n",
    "            red = np.sum(img[:,:,0])\n",
    "            green = np.sum(img[:,:,1])\n",
    "            blue = np.sum(img[:,:,2])\n",
    "            colorsum = green + blue + red\n",
    "            # cluster num is -1 for the original\n",
    "            xcc_over_time.append([[red/colorsum, green/colorsum, blue/colorsum], fog, is_missing_day, -1])\n",
    "        \n",
    "        # for the number of clusters\n",
    "        for current_cluster in ordered_center_color_index:\n",
    "            if isBlack or is_missing_day:\n",
    "                xcc_over_time.append([[ np.nan, np.nan, np.nan], fog, is_missing_day, current_cluster])\n",
    "            else:\n",
    "                masked_image = color_mask_image (clustered_image, \n",
    "                                  labels, \n",
    "                                  current_cluster, \n",
    "                                  img, DILATE)\n",
    "                masked_image_array = np.uint8(masked_image)\n",
    "                red = np.sum(masked_image[:,:,0])\n",
    "                green = np.sum(masked_image[:,:,1])\n",
    "                blue = np.sum(masked_image[:,:,2])\n",
    "                colorsum = green + blue + red\n",
    "                xcc_over_time.append([[red/colorsum, green/colorsum, blue/colorsum], fog, is_missing_day, current_cluster])\n",
    "\n",
    "        i = i+1    \n",
    "    return xcc_over_time\n",
    "\n",
    "# use the \"fog\" flag to signal a data point that should be replaced with interpolation\n",
    "# Function to interpolate values\n",
    "# Helper function to interpolate values\n",
    "####################################################################\n",
    "def interpolate_values(a, b, ratio):\n",
    "    return [x + ratio * (y - x) for x, y in zip(a, b)]\n",
    "\n",
    "####################################################################\n",
    "def interpolate_bad_values_xcc (data):\n",
    "    previous_good_index = None\n",
    "    for i in range(len(data)):\n",
    "        if data[i][1] or data[i][2]:  # If the row has bad data\n",
    "            # Find the index of the next good data row\n",
    "            next_good_index = next((j for j in range(i + 1, len(data)) if not data[j][1] and not data[j][2]), None)\n",
    "            #print (i, \"next good data index\", next_good_index)\n",
    "            if i == 0: # the first element was bad\n",
    "                data[0][0] = data[next_good_index][0]\n",
    "                previous_good_index = 0\n",
    "                #print (\"First row bad data replaced with next_good_index: \", next_good_index)\n",
    "            elif next_good_index is None:\n",
    "                # If no good data row is found until the end, replace with the first column of the last good data\n",
    "                #print (i, \"next good data is none. Last good index: \", previous_good_index)\n",
    "                if previous_good_index is not None:\n",
    "                   # print(\"previous_good_index: \", previous_good_index)\n",
    "                    data[i][0] = data[previous_good_index][0]\n",
    "            else:\n",
    "                # If a good data row is found before the end, replace with the mean of first columns\n",
    "                mean_data = [(a + b) / 2 for a, b in zip(data[previous_good_index][0], data[next_good_index][0])]\n",
    "                data[i][0] = mean_data\n",
    "                #print(\"i mean data =\",i , mean_data, \"last good data\", previous_good_index)\n",
    "\n",
    "        elif previous_good_index is None:\n",
    "            # If the row has good data, update previous_good_index\n",
    "            previous_good_index = i\n",
    "        else: # this row is good data\n",
    "            #print(i, \" good data\")\n",
    "            previous_good_index = i\n",
    "\n",
    "    return data\n",
    "\n",
    "# Creat a mask from one of the cluster color image, \n",
    "#    labels = each pixel is given the index of it's color class, \n",
    "#    color_index_list = the index numbers of the color categories to be used in the mask from KMeans a list of cluster indexes.\n",
    "#    target_image = a new image to be masked\n",
    "# return the modified image with all image not in the masked area set to 0.\n",
    "# Set pixels outside the despeckled mask to zero (black)\n",
    "\n",
    "####################################################################\n",
    "def color_mask_image (clustered_image, labels, desired_clusters_list, target_image, dilate):\n",
    "    # reshape on the x and y axis to list\n",
    "    labels_reshaped = np.reshape(labels, (clustered_image.shape[0], clustered_image.shape[1]))\n",
    "\n",
    "    # Construct the mask by changing any labels not in the desired list to FALSE\n",
    "    mask = np.isin(labels_reshaped, desired_clusters_list)\n",
    "\n",
    "    if dilate == 1:\n",
    "        # erosion eliminated much of the picture because the areas are so fragmented\n",
    "        # Define the structuring element for dilation\n",
    "        kernel = np.ones((3, 3), np.uint8)  # Adjust the kernel size based on desired expansion\n",
    "        # Perform erosion to decrease cluster size\n",
    "        #eroded_mask = cv2.erode(mask, kernel, iterations=1)\n",
    "        #mask = eroded_mask\n",
    "\n",
    "        # Perform dilation to increase cluster size\n",
    "        dilated_mask = dilate_mask(mask, iterations=1)\n",
    "        mask = dilated_mask\n",
    "    \n",
    "    # Apply the mask to the second image\n",
    "    result = target_image.copy()\n",
    "    result[~mask] = 0  # Set pixels outside the mask to zero (black)\n",
    "\n",
    "    # Apply the mask to the second image\n",
    "    #return im.fromarray(result)\n",
    "    return result\n",
    "\n",
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "####################################################################\n",
    "def dilate_mask(mask, iterations=1):\n",
    "    dilated_mask = binary_dilation(mask, iterations=iterations)\n",
    "    return dilated_mask\n",
    "\n",
    "\n",
    "\n",
    "####################################################################\n",
    "def subtract_xcc (xcc, xcc2):\n",
    "# Assuming xcc and xcc2 have the same length\n",
    "    result = []\n",
    "\n",
    "    for data1, data2 in zip(xcc, xcc2):\n",
    "        # Subtract corresponding elements\n",
    "        subtracted_data = [\n",
    "            data1[0][0] - data2[0][0],\n",
    "            data1[0][1] - data2[0][1],\n",
    "            data1[0][2] - data2[0][2]\n",
    "        ]\n",
    "\n",
    "        # Preserve the other columns\n",
    "        result.append([subtracted_data, data1[1], data1[2]])\n",
    "    return result\n",
    "\n",
    "####################################################################\n",
    "#Calculate a running median with a window of 5 to smooth the curves.\n",
    "import statistics\n",
    "def xcc_median5_smooth(xcc):\n",
    "# this should be generalized for any mrcdian window.\n",
    "    window_size = 5\n",
    "    new_first_column = []\n",
    "\n",
    "    for i in range(len(xcc)):\n",
    "        if i == 0:\n",
    "            # Calculate the median for the first two rows\n",
    "            median_1 = statistics.median([data[0][0] for data in xcc[:i+5]])\n",
    "            #print(\"i, median_1, xcc[:1+5]\", i, median_1, xcc[:1+5])\n",
    "            median_2 = statistics.median([data[0][1] for data in xcc[:i+5]])\n",
    "            median_3 = statistics.median([data[0][2] for data in xcc[:i+5]])\n",
    "        elif i == 1:\n",
    "            # Calculate the median for the first two rows\n",
    "            median_1 = statistics.median([data[0][0] for data in xcc[:i+4]])\n",
    "            median_2 = statistics.median([data[0][1] for data in xcc[:i+4]])\n",
    "            median_3 = statistics.median([data[0][2] for data in xcc[:i+4]])\n",
    "        elif i == len(xcc) - 2:\n",
    "            # Calculate the median for the last two rows\n",
    "            median_1 = statistics.median([data[0][0] for data in xcc[i-3:]])\n",
    "            median_2 = statistics.median([data[0][1] for data in xcc[i-3:]])\n",
    "            median_3 = statistics.median([data[0][2] for data in xcc[i-3:]])\n",
    "        elif i == len(xcc) - 1:\n",
    "            # Calculate the median for the last two rows\n",
    "            median_1 = statistics.median([data[0][0] for data in xcc[i-4:]])\n",
    "            median_2 = statistics.median([data[0][1] for data in xcc[i-4:]])\n",
    "            median_3 = statistics.median([data[0][2] for data in xcc[i-4:]])\n",
    "        else:\n",
    "            # Calculate the median for the window of five rows\n",
    "            median_1 = statistics.median([data[0][0] for data in xcc[i-2:i+2]])\n",
    "            median_2 = statistics.median([data[0][1] for data in xcc[i-2:i+2]])\n",
    "            median_3 = statistics.median([data[0][2] for data in xcc[i-2:i+2]])\n",
    "            \n",
    "        new_first_column.append([[median_1, median_2, median_3], xcc[i][1], xcc[i][2]])\n",
    "        \n",
    "    return new_first_column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "# get all date between [start_date, end_date]\n",
    "def get_all_days_in_range(start_date, end_date):\n",
    "    delta = end_date - start_date   # returns timedelta\n",
    "    all_days = []\n",
    "    for i in range(delta.days + 1):\n",
    "        day = start_date + timedelta(days=i)\n",
    "        all_days.append(str(day).replace('-', '_'))\n",
    "    return all_days\n",
    "\n",
    "# get all date in the year\n",
    "def get_all_days_in_month(month, year):\n",
    "    next_month = month + 1 if month < 12 else 1\n",
    "    next_year = year if month < 12 else year+1\n",
    "    all_days =  get_all_days_in_range(date(year, month, 1), date(next_year, next_month, 1))\n",
    "    all_days.pop()\n",
    "\n",
    "    return set(all_days)\n",
    "\n",
    "def append_file_prefix_for_missing_day(missing_days, site, year, month):\n",
    "    return [f'../data_raw/{site}/{year}/{month}/{site}_{missing_day}' for missing_day in missing_days]\n",
    "\n",
    "# find missing day in a year based on sorted days\n",
    "def find_missing_day_in_year(sortedDays, year):\n",
    "    missing_date = []\n",
    "    day_index = 0\n",
    "    all_day_in_year = get_all_days_in_year(YEAR)\n",
    "    for img_file in img_files:\n",
    "        while day_index < len(all_day_in_year) - 1 and all_day_in_year[day_index] not in img_file:\n",
    "            missing_date.append(all_day_in_year[day_index])\n",
    "            day_index += 1\n",
    "        day_index += 1\n",
    "    return missing_date\n",
    "\n",
    "def find_day_in_filename(filename):\n",
    "    if filename.endswith(\"jpg\"):\n",
    "        return filename[-21:-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def get_hazy_days(sitename, year):\n",
    "    hazy_days = []\n",
    "    hazy_days_file = f'../data_out/foggy/{sitename}/{year}.csv'\n",
    "    with open(hazy_days_file, mode='r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for line in reader:\n",
    "            hazy_days.append(f'../data_raw/{line[\"file\"]}')\n",
    "    return hazy_days\n",
    "    \n",
    "# hazy_days = get_hazy_days(\"NEON.D01.BART.DP1.00033\", 2018)\n",
    "# print(hazy_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4d97be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readimg():\n",
    "    \"\"\"\n",
    "        This function will read and show the image and also return it\n",
    "    \"\"\"\n",
    "    image = plt.imread('../data_raw/NEON.D01.HARV.DP1.00033/2017/01/NEON.D01.HARV.DP1.00033_2017_01_31_120006.jpg')\n",
    "#    image = plt.imread('../data_raw/Red,_green_and_blue,_overlapping_basic_colors.jpg')\n",
    "    plt.figure(figsize = (10, 8))\n",
    "    plt.imshow(image)\n",
    "    return image\n",
    "pic_array = readimg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae3364-3b7c-4085-90b0-23a7ae8745cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image as im\n",
    "\n",
    "\n",
    "img_array = pic_array\n",
    "# tried excess green calculation but it did not help.\n",
    "# excess_green_img = calculate_excess_green(img_array)\n",
    "# plt.figure(figsize = (10, 8))\n",
    "# plt.imshow(excess_green_img)\n",
    "#img_array = excess_green_img\n",
    "\n",
    "# Reshape the array to a 2D array of pixels (rows) by color channels (columns)\n",
    "OneDpixelsbycolor = img_array.reshape(-1, 3)\n",
    "\n",
    "# Set the number of clusters\n",
    "n_clusters = 8\n",
    "\n",
    "# Perform K-means clustering on the pixel data\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=10).fit(OneDpixelsbycolor)\n",
    "fit_model = kmeans.fit(OneDpixelsbycolor)\n",
    "# Replace each pixel's color values with the corresponding cluster center\n",
    "new_pixel_colors = np.array([kmeans.cluster_centers_[label] for label in kmeans.labels_])\n",
    "\n",
    "# Reshape the pixel array back into the original image shape\n",
    "Kmean_clustered_img = new_pixel_colors.reshape(img_array.shape)\n",
    "\n",
    "#swap2images(pic_array, np.uint8(Kmean_clustered_img))\n",
    "\n",
    "# Convert the pixel array back to an image\n",
    "clustered_img = im.fromarray(np.uint8(Kmean_clustered_img))\n",
    "\n",
    "# Save the new image\n",
    "clustered_img.save(\"example_clustered.jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63c67e-4c3a-46fe-8a67-9e9d6206e128",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module will present a pie chart of the relative frequency of pixels of the center colors \n",
    "of the kmeans clusters.\n",
    "\"\"\"\n",
    "center_colors = np.uint8(kmeans.cluster_centers_)\n",
    "\n",
    "# Get the labels of the K-means clusters\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Reduce to a 2D array.. just a list of tuples\n",
    "twoDuint_img_array = np.uint8(Kmean_clustered_img.reshape(-1,3))\n",
    "swap2images(pic_array, np.uint8(Kmean_clustered_img))\n",
    "\n",
    "# Get the frequencies of the center colors\n",
    "unique_colors, color_counts = np.unique(twoDuint_img_array, axis=0, return_counts=True)\n",
    "\n",
    "rgb_ordered_colors = ColorClusterPie(labels, center_colors)\n",
    "# Create an array of indices indicating the matching color in color_centers\n",
    "ordered_center_color_index = indices = np.where(np.all(center_colors == rgb_ordered_colors[:, np.newaxis], axis=-1))[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c5fc3-e9e7-4e56-9b98-ad4ed9dc7467",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Turn the snow and sky black\n",
    "color_replaced_snow = replaceAColor (Kmean_clustered_img, \n",
    "                                       rgb_ordered_colors[6], \n",
    "                                       np.array([10, 10, 10], dtype=np.uint8))\n",
    "\n",
    "color_replaced_image = replaceAColor (color_replaced_snow, \n",
    "                                       rgb_ordered_colors[7], \n",
    "                                       np.array([10, 10, 10], dtype=np.uint8))\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.imshow(color_replaced_image)\n",
    "#swap2images(pic_array, color_replaced_image)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e509a8-29c8-42e7-a3aa-a110bacf327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "color_replaced_imageExcept0 = replaceAllButOneColor(Kmean_clustered_img, \n",
    "                       rgb_ordered_colors[0], \n",
    "                       np.array([254, 254, 254], dtype=np.uint8))\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.imshow(color_replaced_imageExcept0)    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9ddc1-e921-46bd-8a46-f74cd574e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "color_replaced_imageExcept1 = replaceAllButOneColor(Kmean_clustered_img, \n",
    "                       rgb_ordered_colors[6], \n",
    "                       np.array([0, 0, 0], dtype=np.uint8))\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.imshow(color_replaced_imageExcept1)  \n",
    "color_replaced_image2 = replaceAColor (Kmean_clustered_img, rgb_ordered_colors[2], np.array([254, 254, 254], dtype=np.uint8))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa7a88-16ae-4ec1-a921-c2b7da016c7e",
   "metadata": {},
   "source": [
    "“markdownCellConfig”:{“fontSize”: 20}}\n",
    "Calculate which masks help, hurt or do nothing to the gcc calculations median \n",
    "during greenup.\n",
    "\n",
    "Calculate the color shifts for each color segment for RCC, GCC and synthedtic colors... maybe merge 3-7 images, clluster, advance in time a day and repeate. calulate nearest neighbor colors in color space to maintain identifty.\n",
    "\n",
    "1) Read in the images\n",
    "2) Calculate GCC using Linh moving median for 30? days = GCCBaseline\n",
    "3) For each cluster\n",
    "    Apply the mask of cluster to the image\n",
    "    Recalculate GCC moving median\n",
    "    Calculate and save the mean difference between GCCBaseline and the masked image\n",
    "4) Sort all differenceGCC. Make a supermask of all that help GCC\n",
    "5) Rerun difference in BaselineGcc to the polymasked\n",
    "\n",
    "\n",
    "? gcc, rcc and bcc normalize brightness. When we start using other colors we'll also need to normalize for brightness. It might be as easy as recalculating gcc, rcc and bcc for each pixel and replacing the original RGB with the normalized values. Or do a regression line of total brightness as spring advnaces. adjust color lines appropriatly... prevent algorithm from learning that the sun gets higher in the sky in summer.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e312576-932e-44c5-a4b1-e02a975b113b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# get all image file names\n",
    "# this needs to be extended to add many years and months\n",
    "RED = 0\n",
    "GREEN = 1\n",
    "BLUE = 2\n",
    "image_names = []\n",
    "hsv_images = []\n",
    "all_hazy_days = []\n",
    "years = [\"2017\"]\n",
    "months = [\"01\" , \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "NEONsite = [\"NEON.D01.HARV.DP1.00033\"]\n",
    "relative_path = \"../data_raw/\"\n",
    "import os\n",
    "for site in NEONsite:\n",
    "    for year in years:\n",
    "        hazy_days = get_hazy_days(site, year)\n",
    "        all_hazy_days.extend(hazy_days)\n",
    "        for month in months:\n",
    "            all_days_in_month = get_all_days_in_month(int(month), int(year))\n",
    "            filepath = os.path.join(\"../data_raw/\", site, year, month, \"\")\n",
    "            for file in os.listdir(filepath):\n",
    "                if file.endswith(\".jpg\"):\n",
    "                    file_date = find_day_in_filename(file)\n",
    "                    all_days_in_month.remove(file_date)\n",
    "                    image_names.append(os.path.join(filepath, file))\n",
    "            image_names.extend(append_file_prefix_for_missing_day(all_days_in_month, site, year, month))\n",
    "image_names.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f'missing days: {[img for img in image_names if not img.endswith(\".jpg\")]}\\n\\n')\n",
    "print(f'hazy days: {all_hazy_days}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xcc_over_time = get_xcc(image_names, Kmean_clustered_img, \n",
    "                        labels, ordered_center_color_index, all_hazy_days)\n",
    "xcc_original = [item for item in xcc_over_time if item[-1] == -1]\n",
    "xcc_original_data = [item[0] for item in xcc_original]\n",
    "interpolated_xcc_original = interpolate_bad_values_xcc (xcc_original)\n",
    "\n",
    "xcc_cluster = []\n",
    "interpolated_xcc_cluster = []\n",
    "for current_cluster in range (0, n_clusters):\n",
    "    ordered_center_color_index[current_cluster]\n",
    "    xcc_cluster.append([item for item in xcc_over_time if item[-1] == current_cluster])\n",
    "    interpolated_xcc_cluster.append(interpolate_bad_values_xcc (xcc_cluster[current_cluster]))\n",
    "\n",
    "# Save the xcc list to a file\n",
    "import pickle\n",
    " \n",
    "with open(\"\".join([\"../data_raw/\", NEONsite[0], years[0], \"xcc_over_time.pkl\"]), 'wb') as file:\n",
    "    pickle.dump(xcc_over_time, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing = [x for x in xcc_original if x[2]]\n",
    "print(len(missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc0c693-348b-4efc-bfb6-8edf09048def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the xcc data in binary and csv formats. \n",
    "# Construct the file name from the location + first year + last year + first month + last month\n",
    "\n",
    "xcc_file_name_body = f\"{relative_path}{NEONsite[0]}_xcc{years[0]}_{years[-1]}_{months[0]}_{months[-1]}\"\n",
    "xcc_file_name_pkl = f\"{xcc_file_name_body}.pkl\"\n",
    "xcc_file_name_csv = f\"{xcc_file_name_body}.csv\"\n",
    "\n",
    "import csv\n",
    "# Save xcc to a CSV file\n",
    "with open(xcc_file_name_csv, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for row in xcc_over_time:\n",
    "        writer.writerow(row)\n",
    "\n",
    "import pickle\n",
    "# Save xcc to a binary file\n",
    "with open(xcc_file_name_pkl, 'wb') as file:\n",
    "    pickle.dump(xcc_over_time, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70bf91f-1f59-4ece-8402-170e80c0fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code would read the data back in... it is not needed in this module.\n",
    "# this is just for later use.\n",
    "xcc_file_name_body = f\"{relative_path}{NEONsite[0]}_xcc{years[0]}_{years[-1]}_{months[0]}_{months[-1]}\"\n",
    "xcc_file_name_pkl = f\"{xcc_file_name_body}.pkl\"\n",
    "# Specify the CSV file path and name\n",
    "xcc_file_name_csv = f\"{xcc_file_name_body}.csv\"\n",
    "\n",
    "import csv\n",
    "# Load xcc from the CSV file\n",
    "xcc = []\n",
    "with open(xcc_file_name_csv, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        xcc_over_time.append([float(value) for value in row])\n",
    "\n",
    "# xcc is now restored with the data from the CSV file\n",
    "# Load xcc from the file\n",
    "import pickle\n",
    "with open(xcc_file_name_pkl, 'rb') as file:\n",
    "    xcc_over_time = pickle.load(file)\n",
    "\n",
    "# xcc is now restored with the saved data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01182784-3070-4a45-8233-2a4ce5e4a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xcc_caption = str(years[0]) + \"-\" + years[-1] + \" Day Number (some days may be skipped)\"\n",
    "smoothed_xcc_original = xcc_median5_smooth(interpolated_xcc_original)\n",
    "plt.figure(figsize = (10, 8))\n",
    "plot_xcc(xcc_median5_smooth(interpolated_xcc_original), \"Full Image rcc, gcc, bcc\", xcc_caption)\n",
    "summer_image = plt.imread('../data_raw/NEON.D01.HARV.DP1.00033/2017/06/NEON.D01.HARV.DP1.00033_2017_06_30_120006.jpg')\n",
    "\n",
    "DILATE = 1\n",
    "# for current_cluster in range (0, n_clusters):\n",
    "for current_cluster in ordered_center_color_index:\n",
    "    masked_image = color_mask_image (Kmean_clustered_img, \n",
    "                        labels, \n",
    "                        [current_cluster], \n",
    "                        summer_image, DILATE)\n",
    "    xcc_title = \"Filter#\" + str(current_cluster) +  \" Magnitude of rcc, gcc, and bcc\"\n",
    "    plt.figure(figsize = (10, 8))\n",
    "    plot_xcc(xcc_median5_smooth(interpolated_xcc_cluster[current_cluster]), xcc_title, xcc_caption)\n",
    "    plt.imshow(masked_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ad5eed-5ae8-4408-a40a-059abc24ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masked_image)\n",
    "#del selected_rows\n",
    "#del PhenoEndDOY\n",
    "#del PhenoEndDate\n",
    "#del PhenoStartDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a7aa2-9c8b-44d5-91f7-7103ed77a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# This module reads USA-NPN Site phenometrics files for a given year and extracts the names\n",
    "# of tree species that were observed that year.\n",
    "# Colors are assigned to each species based on a prior color list.\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "NPNFile = \"../data_raw/NPNNEON.D01.HARV.DP1.000332017-2023datasheet_1685753626088/site_phenometrics_dataHARV2017-2023.csv\"\n",
    "NPNdf = pd.read_csv(NPNFile)\n",
    "\n",
    "# Create a dictionary to store the unique species_IDs along with their paired Common_Names\n",
    "filtered_data = NPNdf[NPNdf['Mean_First_Yes_Year'] == 2017]\n",
    "\n",
    "species_dict = {}\n",
    "\n",
    "# Iterate through the DataFrame to populate the dictionary\n",
    "for index, row in filtered_data.iterrows():\n",
    "    species_id = row['Species_ID']\n",
    "    common_name = row['Common_Name']\n",
    "    species_dict[species_id] = common_name\n",
    "unique_species_list = list(species_dict.items())\n",
    "cmap = cm.get_cmap('tab10') # Use a color map for getting colors\n",
    "SpeciesList = [] # Create a new list with the merged colors\n",
    "\n",
    "for i, (species_id, species_name) in enumerate(unique_species_list):\n",
    "    # Get the color from the color map based on the index 'i'\n",
    "    color = cmap(i)\n",
    "    SpeciesList.append((species_id, species_name, color))\n",
    "#print (SpeciesList)\n",
    "\n",
    "# Merge PhenophaseList and linestyles into one two-dimensional list\n",
    "# Phenophase codes of interest\n",
    "phenophase_dict = {}\n",
    "\n",
    "# Iterate through the DataFrame to populate the dictionary\n",
    "for index, row in filtered_data.iterrows():\n",
    "    phenophase_id = row['Phenophase_ID']\n",
    "    phenophase_description = row['Phenophase_Description']\n",
    "    phenophase_dict[phenophase_id] = phenophase_description\n",
    "unique_phenophase_list = list(phenophase_dict.items())\n",
    "\n",
    "'''\n",
    "linestyles = [\n",
    "    #(lines style, thickness)\n",
    "    (\"solid\", 1),\n",
    "    (0, (3, 1, 1, 1, 1, 1), 1),  # Densely DashDotted\n",
    "    (\"dotted\", 1),\n",
    "    (\"dashdot\", 1),\n",
    "    ((0, (1, 1)), 1),  # Loosely dotted\n",
    "    (\"dashed\", 1)\n",
    "    (\"solid\", 3),\n",
    "    (0, (3, 1, 1, 1, 1, 1), 3),  # Densely DashDotted\n",
    "    (\"dotted\", 3),\n",
    "    (\"dashdot\", 3),\n",
    "    ((0, (1, 1)), 3),  # Loosely dotted\n",
    "    (\"dashed\")\n",
    "]\n",
    "'''\n",
    "linestyle_tuple = [\n",
    "    ('solid',        (0, ()),              1),\n",
    "    ('loosely dotted',        (0, (1, 10)),  1),\n",
    "    ('dotted',                (0, (1, 1)),   1),\n",
    "    ('densely dotted',        (0, (1, .5)),  1),\n",
    "    ('long dash with offset', (5, (10, 3)),  1),\n",
    "    ('loosely dashed',        (0, (5, 10)),  1),\n",
    "    ('dashed',                (0, (4, 3)),   1),\n",
    "    ('densely dashed',        (0, (5, 1)),   1),\n",
    "    ('loosely dashdotted',    (0, (3, 10, 1, 10)), 1),\n",
    "    ('dashdotted',            (0, (3, 5, 1, 5)),    1),\n",
    "    ('densely dashdotted',    (0, (3, 1, 1, 1)),    1),\n",
    "    ('dashdotdotted',         (0, (3, 5, 1, 5, 1, 5)), 1),\n",
    "    ('loosely dashdotdotted', (0, (3, 10, 1, 10, 1, 10)), 1),\n",
    "    ('densely dashdotdotted', (0, (3, 1, 1, 1, 1, 1)), 1),\n",
    "    ('thick solid',        (0, ()),              3),\n",
    "    ('thick loosely dotted',        (0, (1, 10)),  3),\n",
    "    ('thick dotted',                (0, (1, 1)),   3),\n",
    "    ('thick densely dotted',        (0, (1, .5)),  3),\n",
    "    ('thick long dash with offset', (5, (10, 3)),  3),\n",
    "    ('thick loosely dashed',        (0, (5, 10)),  3),\n",
    "    ('thick dashed',                (0, (4, 3)),   3),\n",
    "    ('thick densely dashed',        (0, (5, 1)),   3),\n",
    "    ('thick loosely dashdotted',    (0, (3, 10, 1, 10)), 3),\n",
    "    ('thick dashdotted',            (0, (3, 5, 1, 5)),    3),\n",
    "    ('thick densely dashdotted',    (0, (3, 1, 1, 1)),    3),\n",
    "    ('thick dashdotdotted',         (0, (3, 5, 1, 5, 1, 5)), 3),\n",
    "    ('thick loosely dashdotdotted', (0, (3, 10, 1, 10, 1, 10)), 3),\n",
    "    ('thick densely dashdotdotted', (0, (3, 1, 1, 1, 1, 1)), 3)\n",
    "]\n",
    "# Convert the list of tuples to a list of dictionaries with line styles as keys\n",
    "linestyles = [{ 'linestyle': style, 'dash_pattern': dash, 'linewidth': width } \n",
    "              for style, dash, width in linestyle_tuple]\n",
    "\n",
    "PhenophaseList = [] # Create a new list with the merged colors\n",
    "for i, (phenophase_id, phenophase_description) in enumerate(unique_phenophase_list):\n",
    "    # Get the linestyle and thickness from the color map based on the index 'i'\n",
    "    #linestyle = linestyles(i)\n",
    "    linestyle = linestyles[i]['linestyle']\n",
    "    dash_pattern = linestyles[i]['dash_pattern']\n",
    "    linewidth = linestyles[i]['linewidth']\n",
    "    PhenophaseList.append((species_id, phenophase_description, linestyle, linewidth))\n",
    "#    plt.plot(x, y, lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e7b045-a589-47f1-8e44-2526921c6a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# This module reads USA-NPN Site phenometrics files for a given year and extracts the names\n",
    "# of tree species that were observed that year.\n",
    "# Colors are assigned to each species based on a prior color list.\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "NPNFile = '../data_raw/NPNNEON.D01.HARV.DP1.000332017-2023datasheet_1685753626088/site_phenometrics_dataHARV2017-2023.csv'\n",
    "NPNdf = pd.read_csv(NPNFile)\n",
    "\n",
    "# Create a dictionary to store the unique species_IDs along with their paired Common_Names\n",
    "filtered_data = NPNdf[NPNdf['Mean_First_Yes_Year'] == 2017]\n",
    "\n",
    "species_dict = {}\n",
    "\n",
    "# Iterate through the DataFrame to populate the dictionary\n",
    "for index, row in filtered_data.iterrows():\n",
    "    species_id = row['Species_ID']\n",
    "    common_name = row['Common_Name']\n",
    "    species_dict[species_id] = common_name\n",
    "unique_species_list = list(species_dict.items())\n",
    "cmap = cm.get_cmap('tab10') # Use a color map for getting colors\n",
    "SpeciesList = [] # Create a new list with the merged colors\n",
    "\n",
    "for i, (species_id, species_name) in enumerate(unique_species_list):\n",
    "    # Get the color from the color map based on the index 'i'\n",
    "    color = cmap(i)\n",
    "    SpeciesList.append((species_id, species_name, color))\n",
    "#print (SpeciesList)\n",
    "\n",
    "# Merge PhenophaseList and linestyles into one two-dimensional list\n",
    "# Phenophase codes of interest\n",
    "phenophase_dict = {}\n",
    "\n",
    "# Iterate through the DataFrame to populate the dictionary\n",
    "for index, row in filtered_data.iterrows():\n",
    "    phenophase_id = row['Phenophase_ID']\n",
    "    phenophase_description = row['Phenophase_Description']\n",
    "    phenophase_dict[phenophase_id] = phenophase_description\n",
    "unique_phenophase_list = list(phenophase_dict.items())\n",
    "\n",
    "\n",
    "linestyle_tuple = [\n",
    "    ('solid',        (0, ()),              1),\n",
    "    ('loosely dotted',        (0, (1, 5)),  1),\n",
    "    ('dotted',                (0, (1, 2)),   1),\n",
    "    ('densely dotted',        (0, (1, .5)),  1),\n",
    "    ('long dash with offset', (2.5, (10, 3)),  1),\n",
    "    ('loosely dashed',        (0, (5, 10)),  1),\n",
    "    ('dashed',                (0, (4, 3)),   1),\n",
    "    ('densely dashed',        (0, (5, 1)),   1),\n",
    "    ('loosely dashdotted',    (0, (3, 10, 1, 10)), 1),\n",
    "    ('dashdotted',            (0, (3, 5, 1, 5)),    1),\n",
    "    ('densely dashdotted',    (0, (3, 1, 1, 1)),    1),\n",
    "    ('dashdotdotted',         (0, (3, 5, 1, 5, 1, 5)), 1),\n",
    "    ('loosely dashdotdotted', (0, (3, 10, 1, 10, 1, 10)), 1),\n",
    "    ('densely dashdotdotted', (0, (3, 1, 1, 1, 1, 1)), 1),\n",
    "    ('thick solid',        (0, ()),              3),\n",
    "    ('thick loosely dotted',        (0, (1, 10)),  3),\n",
    "    ('thick dotted',                (0, (1, 1)),   3),\n",
    "    ('thick densely dotted',        (0, (1, .5)),  3),\n",
    "    ('thick long dash with offset', (5, (10, 3)),  3),\n",
    "    ('thick loosely dashed',        (0, (5, 10)),  3),\n",
    "    ('thick dashed',                (0, (4, 3)),   3),\n",
    "    ('thick densely dashed',        (0, (5, 1)),   3),\n",
    "    ('thick loosely dashdotted',    (0, (3, 10, 1, 10)), 3),\n",
    "    ('thick dashdotted',            (0, (3, 5, 1, 5)),    3),\n",
    "    ('thick densely dashdotted',    (0, (3, 1, 1, 1)),    3),\n",
    "    ('thick dashdotdotted',         (0, (3, 5, 1, 5, 1, 5)), 3),\n",
    "    ('thick loosely dashdotdotted', (0, (3, 10, 1, 10, 1, 10)), 3),\n",
    "    ('thick densely dashdotdotted', (0, (3, 1, 1, 1, 1, 1)), 3)\n",
    "]\n",
    "# Convert the list of tuples to a list of dictionaries with line styles as keys\n",
    "linestyles = [{ 'linestyle': style, 'dash_pattern': dash, 'linewidth': width } \n",
    "              for style, dash, width in linestyle_tuple]\n",
    "\n",
    "PhenophaseList = [] # Create a new list with the merged colors\n",
    "for i, (phenophase_id, phenophase_description) in enumerate(unique_phenophase_list):\n",
    "    linestyle = linestyles[i]['linestyle']\n",
    "    dash_pattern = linestyles[i]['dash_pattern']\n",
    "    linewidth = linestyles[i]['linewidth']\n",
    "    PhenophaseList.append((phenophase_id, phenophase_description, dash_pattern, linewidth))\n",
    "print(PhenophaseList)\n",
    "smoothed_xcc_original = xcc_median5_smooth(interpolated_xcc_original)\n",
    "\n",
    "rcc = [data[0][0] for data in smoothed_xcc_original]\n",
    "rcc_caption = str(years[0]) + \"-\" + years[-1] + \" Day Number missing data linearly interpolated\"\n",
    "#xcc_title = \"Filter#\" + str(current_cluster) +  \" Magnitude of rcc, gcc, and bcc\"\n",
    "rcc_title = \"Full Image Magnitude of rcc and observed phenophase date range\"\n",
    "x = range(len(rcc))\n",
    "\n",
    "#get foggy point and not available point\n",
    "foggy_points = [data[1] for data in smoothed_xcc_original]\n",
    "missing_points = [data[2] for data in smoothed_xcc_original]\n",
    "\n",
    "Mean_First_Yes_Year  = 'Mean_First_Yes_Year'\n",
    "# we might consider using , mean or median\n",
    "PhenophaseStartDateColName = 'Mean_First_Yes_DOY'\n",
    "PhenophaseEndDateColName = 'Mean_Last_Yes_DOY'\n",
    "# Plot the lines\n",
    "# there could be a larger outter loop for years \n",
    "Obsyear = 2017\n",
    "# Obsyear = 2021\n",
    "# Generate x-axis values and tick labels for the first day of each month\n",
    "x_values = []\n",
    "x_tick_positions = []\n",
    "x_tick_labels = []\n",
    "is_leap_year = calendar.isleap(Obsyear)\n",
    "num_days_in_february = 29 if is_leap_year else 28\n",
    "    \n",
    "for month in range(1, 13):\n",
    "    num_days_in_month = calendar.monthrange(Obsyear, month)[1]\n",
    "        \n",
    "    if month == 1:\n",
    "        x_values.append(0)\n",
    "        x_tick_positions.append(len(x_values) - 1)\n",
    "        x_tick_labels.append(calendar.month_abbr[month])\n",
    "    else:\n",
    "        x_values.append(x_values[-1] + num_days_in_month)\n",
    "        x_tick_positions.append(len(x_values) - 1)\n",
    "        x_tick_labels.append(calendar.month_abbr[month])\n",
    "        \n",
    "    if month == 2:\n",
    "        x_values[-1] += (num_days_in_february - num_days_in_month)\n",
    "\n",
    "plt.figure(figsize = (15, 8))\n",
    "\n",
    "plt.plot(x, rcc, '-gD', markevery=foggy_points, label='foggy')\n",
    "plt.plot(x, rcc, '-bX', markevery=missing_points, label='missing')\n",
    "plt.plot(x, rcc, color='red', label='rcc')\n",
    "\n",
    "# Add a legend for phenophase and species\n",
    "phenophase_handles = []\n",
    "phenophase_labels = []\n",
    "\n",
    "for _, phenophase, linestyle, linewidth in PhenophaseList:\n",
    "    line, = plt.plot([], [], linestyle=linestyle, linewidth=linewidth, color='black')\n",
    "    phenophase_handles.append(line)\n",
    "    phenophase_labels.append(phenophase)\n",
    "\n",
    "#since only one legend is managesd at a time it is nexessary to place one manually\n",
    "PhenoLegend = plt.legend(phenophase_handles, phenophase_labels, loc='upper left', title='Phenophase')\n",
    "ax = plt.gca().add_artist(PhenoLegend)\n",
    "\n",
    "# Create legend for species with colored lines\n",
    "species_handles = []\n",
    "species_labels = []\n",
    "\n",
    "for  _, species, color in SpeciesList:\n",
    "    line, = plt.plot([], [], color=color)\n",
    "    species_handles.append(line)\n",
    "    species_labels.append(species)\n",
    "\n",
    "plt.legend(species_handles, species_labels, loc='upper right', title='Species')\n",
    "\n",
    "for speciesID, speciesname, SpeciesColor in SpeciesList:\n",
    "    #print( speciesname, speciesID, SpeciesColor )\n",
    "    for  PhenoCode, PhenoName, PhenoLineStyle, linewidth in PhenophaseList:\n",
    "        #print('PhenoName, PhenoCode, PhenoLineStyle, linewidth ' \n",
    "        #      + PhenoName, PhenoCode, PhenoLineStyle, linewidth)\n",
    "        selected_rows = NPNdf[(NPNdf['Species_ID'] == speciesID)\n",
    "                & (NPNdf[Mean_First_Yes_Year] == Obsyear) \n",
    "                   & (NPNdf['Phenophase_ID'] == PhenoCode)]\n",
    "        #print (selected_rows)\n",
    "        if selected_rows.empty == False:\n",
    "            PhenoStartDOY = list(selected_rows[PhenophaseStartDateColName])\n",
    "            PhenoEndDOY = list(selected_rows[PhenophaseEndDateColName])\n",
    "            # if end is before start then it rolled to the next year. reset to 365\n",
    "            if PhenoStartDOY > PhenoEndDOY: PhenoEndDOY = 365\n",
    "            #print(\"start stop\", [PhenoStartDOY[0], PhenoEndDOY[0]], \n",
    "            #      [rcc[PhenoStartDOY[0]], rcc[PhenoStartDOY[0]]], color=SpeciesColor, \n",
    "            #      linestyle=PhenoLineStyle, linewidth=linewidth, label='Flowering')\n",
    "            plt.plot([PhenoStartDOY[0], PhenoEndDOY[0]], \n",
    "                     [rcc[PhenoStartDOY[0]], rcc[PhenoStartDOY[0]]], color=SpeciesColor, \n",
    "                     linestyle=PhenoLineStyle, linewidth=linewidth, label=PhenoName)\n",
    "\n",
    "# Set x-axis tick positions and labels\n",
    "plt.xticks(x_values, x_tick_labels)\n",
    "\n",
    "\n",
    "# Set the axis labels and title\n",
    "plt.xlabel(rcc_caption)\n",
    "plt.ylabel('RCC Magnitude')\n",
    "plt.title(rcc_title)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# This module reads USA-NPN Site phenometrics files for a given year and extracts the names\n",
    "# of tree species that were observed that year.\n",
    "# Colors are assigned to each species based on a prior color list.\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "NPNFile = '../data_raw/NPNNEON.D01.HARV.DP1.000332017-2023datasheet_1685753626088/site_phenometrics_dataHARV2017-2023.csv'\n",
    "NPNdf = pd.read_csv(NPNFile)\n",
    "\n",
    "# Create a dictionary to store the unique species_IDs along with their paired Common_Names\n",
    "filtered_data = NPNdf[NPNdf['Mean_First_Yes_Year'] == 2017]\n",
    "\n",
    "species_dict = {}\n",
    "\n",
    "# Iterate through the DataFrame to populate the dictionary\n",
    "for index, row in filtered_data.iterrows():\n",
    "    species_id = row['Species_ID']\n",
    "    common_name = row['Common_Name']\n",
    "    species_dict[species_id] = common_name\n",
    "unique_species_list = list(species_dict.items())\n",
    "cmap = cm.get_cmap('tab10') # Use a color map for getting colors\n",
    "SpeciesList = [] # Create a new list with the merged colors\n",
    "\n",
    "for i, (species_id, species_name) in enumerate(unique_species_list):\n",
    "    # Get the color from the color map based on the index 'i'\n",
    "    color = cmap(i)\n",
    "    SpeciesList.append((species_id, species_name, color))\n",
    "#print (SpeciesList)\n",
    "\n",
    "# Merge PhenophaseList and linestyles into one two-dimensional list\n",
    "# Phenophase codes of interest\n",
    "phenophase_dict = {}\n",
    "\n",
    "# Iterate through the DataFrame to populate the dictionary\n",
    "for index, row in filtered_data.iterrows():\n",
    "    phenophase_id = row['Phenophase_ID']\n",
    "    phenophase_description = row['Phenophase_Description']\n",
    "    phenophase_dict[phenophase_id] = phenophase_description\n",
    "unique_phenophase_list = list(phenophase_dict.items())\n",
    "\n",
    "\n",
    "linestyle_tuple = [\n",
    "    ('solid',        (0, ()),              1),\n",
    "    ('loosely dotted',        (0, (1, 5)),  1),\n",
    "    ('dotted',                (0, (1, 2)),   1),\n",
    "    ('densely dotted',        (0, (1, .5)),  1),\n",
    "    ('long dash with offset', (2.5, (10, 3)),  1),\n",
    "    ('loosely dashed',        (0, (5, 10)),  1),\n",
    "    ('dashed',                (0, (4, 3)),   1),\n",
    "    ('densely dashed',        (0, (5, 1)),   1),\n",
    "    ('loosely dashdotted',    (0, (3, 10, 1, 10)), 1),\n",
    "    ('dashdotted',            (0, (3, 5, 1, 5)),    1),\n",
    "    ('densely dashdotted',    (0, (3, 1, 1, 1)),    1),\n",
    "    ('dashdotdotted',         (0, (3, 5, 1, 5, 1, 5)), 1),\n",
    "    ('loosely dashdotdotted', (0, (3, 10, 1, 10, 1, 10)), 1),\n",
    "    ('densely dashdotdotted', (0, (3, 1, 1, 1, 1, 1)), 1),\n",
    "    ('thick solid',        (0, ()),              3),\n",
    "    ('thick loosely dotted',        (0, (1, 10)),  3),\n",
    "    ('thick dotted',                (0, (1, 1)),   3),\n",
    "    ('thick densely dotted',        (0, (1, .5)),  3),\n",
    "    ('thick long dash with offset', (5, (10, 3)),  3),\n",
    "    ('thick loosely dashed',        (0, (5, 10)),  3),\n",
    "    ('thick dashed',                (0, (4, 3)),   3),\n",
    "    ('thick densely dashed',        (0, (5, 1)),   3),\n",
    "    ('thick loosely dashdotted',    (0, (3, 10, 1, 10)), 3),\n",
    "    ('thick dashdotted',            (0, (3, 5, 1, 5)),    3),\n",
    "    ('thick densely dashdotted',    (0, (3, 1, 1, 1)),    3),\n",
    "    ('thick dashdotdotted',         (0, (3, 5, 1, 5, 1, 5)), 3),\n",
    "    ('thick loosely dashdotdotted', (0, (3, 10, 1, 10, 1, 10)), 3),\n",
    "    ('thick densely dashdotdotted', (0, (3, 1, 1, 1, 1, 1)), 3)\n",
    "]\n",
    "# Convert the list of tuples to a list of dictionaries with line styles as keys\n",
    "linestyles = [{ 'linestyle': style, 'dash_pattern': dash, 'linewidth': width } \n",
    "              for style, dash, width in linestyle_tuple]\n",
    "\n",
    "PhenophaseList = [] # Create a new list with the merged colors\n",
    "for i, (phenophase_id, phenophase_description) in enumerate(unique_phenophase_list):\n",
    "    linestyle = linestyles[i]['linestyle']\n",
    "    dash_pattern = linestyles[i]['dash_pattern']\n",
    "    linewidth = linestyles[i]['linewidth']\n",
    "    PhenophaseList.append((phenophase_id, phenophase_description, dash_pattern, linewidth))\n",
    "print(PhenophaseList)\n",
    "smoothed_xcc_original = xcc_median5_smooth(interpolated_xcc_original)\n",
    "\n",
    "gcc = [data[0][1] for data in smoothed_xcc_original]\n",
    "gcc_caption = str(years[0]) + \"-\" + years[-1] + \" Day Number missing data linearly interpolated\"\n",
    "#xcc_title = \"Filter#\" + str(current_cluster) +  \" Magnitude of rcc, gcc, and bcc\"\n",
    "gcc_title = \"Full Image Magnitude of gcc and observed phenophase date range\"\n",
    "x = range(len(gcc))\n",
    "\n",
    "#get foggy point and not available point\n",
    "foggy_points = [data[1] for data in smoothed_xcc_original]\n",
    "missing_points = [data[2] for data in smoothed_xcc_original]\n",
    "\n",
    "Mean_First_Yes_Year  = 'Mean_First_Yes_Year'\n",
    "# we might consider using , mean or median\n",
    "PhenophaseStartDateColName = 'Mean_First_Yes_DOY'\n",
    "PhenophaseEndDateColName = 'Mean_Last_Yes_DOY'\n",
    "# Plot the lines\n",
    "# there could be a larger outter loop for years \n",
    "Obsyear = 2017\n",
    "# Obsyear = 2021\n",
    "# Generate x-axis values and tick labels for the first day of each month\n",
    "x_values = []\n",
    "x_tick_positions = []\n",
    "x_tick_labels = []\n",
    "is_leap_year = calendar.isleap(Obsyear)\n",
    "num_days_in_february = 29 if is_leap_year else 28\n",
    "    \n",
    "for month in range(1, 13):\n",
    "    num_days_in_month = calendar.monthrange(Obsyear, month)[1]\n",
    "        \n",
    "    if month == 1:\n",
    "        x_values.append(0)\n",
    "        x_tick_positions.append(len(x_values) - 1)\n",
    "        x_tick_labels.append(calendar.month_abbr[month])\n",
    "    else:\n",
    "        x_values.append(x_values[-1] + num_days_in_month)\n",
    "        x_tick_positions.append(len(x_values) - 1)\n",
    "        x_tick_labels.append(calendar.month_abbr[month])\n",
    "        \n",
    "    if month == 2:\n",
    "        x_values[-1] += (num_days_in_february - num_days_in_month)\n",
    "\n",
    "plt.figure(figsize = (15, 8))\n",
    "\n",
    "plt.plot(x, gcc, '-gD', markevery=foggy_points, label='foggy')\n",
    "plt.plot(x, gcc, '-bX', markevery=missing_points, label='missing')\n",
    "\n",
    "plt.plot(x, gcc, color='green', label='gcc')\n",
    "\n",
    "# Add a legend for phenophase and species\n",
    "phenophase_handles = []\n",
    "phenophase_labels = []\n",
    "\n",
    "for _, phenophase, linestyle, linewidth in PhenophaseList:\n",
    "    line, = plt.plot([], [], linestyle=linestyle, linewidth=linewidth, color='black')\n",
    "    phenophase_handles.append(line)\n",
    "    phenophase_labels.append(phenophase)\n",
    "\n",
    "#since only one legend is managesd at a time it is nexessary to place one manually\n",
    "PhenoLegend = plt.legend(phenophase_handles, phenophase_labels, loc='upper left', title='Phenophase')\n",
    "ax = plt.gca().add_artist(PhenoLegend)\n",
    "\n",
    "# Create legend for species with colored lines\n",
    "species_handles = []\n",
    "species_labels = []\n",
    "\n",
    "for  _, species, color in SpeciesList:\n",
    "    line, = plt.plot([], [], color=color)\n",
    "    species_handles.append(line)\n",
    "    species_labels.append(species)\n",
    "\n",
    "plt.legend(species_handles, species_labels, loc='upper right', title='Species')\n",
    "\n",
    "for speciesID, speciesname, SpeciesColor in SpeciesList:\n",
    "    #print( speciesname, speciesID, SpeciesColor )\n",
    "    for  PhenoCode, PhenoName, PhenoLineStyle, linewidth in PhenophaseList:\n",
    "        #print('PhenoName, PhenoCode, PhenoLineStyle, linewidth ' \n",
    "        #      + PhenoName, PhenoCode, PhenoLineStyle, linewidth)\n",
    "        selected_rows = NPNdf[(NPNdf['Species_ID'] == speciesID)\n",
    "                & (NPNdf[Mean_First_Yes_Year] == Obsyear) \n",
    "                   & (NPNdf['Phenophase_ID'] == PhenoCode)]\n",
    "        #print (selected_rows)\n",
    "        if selected_rows.empty == False:\n",
    "            PhenoStartDOY = list(selected_rows[PhenophaseStartDateColName])\n",
    "            PhenoEndDOY = list(selected_rows[PhenophaseEndDateColName])\n",
    "            # if end is before start then it rolled to the next year. reset to 365\n",
    "            if PhenoStartDOY > PhenoEndDOY: PhenoEndDOY = 365\n",
    "            #print(\"start stop\", [PhenoStartDOY[0], PhenoEndDOY[0]], \n",
    "            #      [rcc[PhenoStartDOY[0]], rcc[PhenoStartDOY[0]]], color=SpeciesColor, \n",
    "            #      linestyle=PhenoLineStyle, linewidth=linewidth, label='Flowering')\n",
    "            plt.plot([PhenoStartDOY[0], PhenoEndDOY[0]], \n",
    "                     [gcc[PhenoStartDOY[0]], gcc[PhenoStartDOY[0]]], color=SpeciesColor, \n",
    "                     linestyle=PhenoLineStyle, linewidth=linewidth, label=PhenoName)\n",
    "\n",
    "# Set x-axis tick positions and labels\n",
    "plt.xticks(x_values, x_tick_labels)\n",
    "\n",
    "\n",
    "# Set the axis labels and title\n",
    "plt.xlabel(gcc_caption)\n",
    "plt.ylabel('GCC Magnitude')\n",
    "plt.title(gcc_title)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([m for m in missing_points if m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675607ca-b2e6-44e8-9c2b-99083f53109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xcc_caption = str(years[0]) + \"-\" + years[-1] + \" Day Number (some days may be skipped)\"\n",
    "smoothed_xcc_original = xcc_median5_smooth(interpolated_xcc_original)\n",
    "plt.figure(figsize = (10, 8))\n",
    "plot_xcc(xcc_median5_smooth(interpolated_xcc_original), \"Full Image rcc, gcc, bcc\", xcc_caption)\n",
    "summer_image = plt.imread('../data_raw/NEON.D01.HARV.DP1.00033/2018/06/NEON.D01.HARV.DP1.00033_2018_06_30_120006.jpg')\n",
    "\n",
    "DILATE = 1\n",
    "for current_cluster in range (0, n_clusters):\n",
    "#for current_cluster in ordered_center_color_index:\n",
    "    masked_image = color_mask_image (Kmean_clustered_img, \n",
    "                        labels, \n",
    "                        [current_cluster], \n",
    "                        summer_image, DILATE)\n",
    "    xcc_title = \"Filter#\" + str(current_cluster) +  \"Diff from Original Magnitude of rcc, gcc, and bcc\"\n",
    "    xcc_diff = subtract_xcc (smoothed_xcc_original, xcc_median5_smooth(interpolated_xcc_cluster[current_cluster]))\n",
    "    plt.figure(figsize = (10, 8))\n",
    "    plot_xcc(xcc_diff, xcc_title, xcc_caption)\n",
    "    plt.imshow(masked_image)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "smoothed_xcc_original = xcc_median5_smooth(interpolated_xcc_original)\n",
    "plot_xcc(xcc_median5_smooth(smoothed_xcc_original), \"Original Iimage rcc, gcc, bcc\")\n",
    "plot_xcc(interpolated_xcc_original, \"Original Images rcc, gcc and bcc\")\n",
    "summer_image = plt.imread('../data_raw/NEON.D01.HARV.DP1.00033/2017/06/NEON.D01.HARV.DP1.00033_2017_06_30_120006.jpg')\n",
    "DILATE = 1\n",
    "for current_cluster in range (0, n_clusters):\n",
    "    masked_image = color_mask_image (Kmean_clustered_img, \n",
    "                        labels, \n",
    "                        [ordered_center_color_index[current_cluster]], \n",
    "                        summer_image, DILATE)\n",
    "    xcc_diff = subtract_xcc (smoothed_xcc_original, xcc_median5_smooth(interpolated_xcc_cluster[current_cluster]))\n",
    "    print(\"cluster #:\", current_cluster)\n",
    "    plt.figure(figsize = (10, 8))\n",
    "    plot_xcc(xcc_diff, \"Difference between whole image and mask rcc, gcc, bcc\")\n",
    "    plt.imshow(masked_image)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e36c2-1abf-4ba4-a911-39e4a59db207",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "new_list_1 = [item for item in xcc_over_time if item[-1] == 6]\n",
    "new_list_2 = [item[0] for item in new_list_1]\n",
    "plt.plot([row[RED] for row in new_list_2], color='red')\n",
    "plt.plot([row[GREEN] for row in new_list_2], color='green')\n",
    "plt.plot([row[BLUE] for row in new_list_2], color='blue')\n",
    "plt.title(\"RCC comparison - with interpolated values\")\n",
    "plt.show()\n",
    "\n",
    "#print (new_list_1)\n",
    "#print (new_list_2)\n",
    "#print(new_list_1[[][0]])\n",
    "\n",
    "i = 1\n",
    "for row in new_list_1:\n",
    "    print(i, row)\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39757e7-793b-4c95-814a-bd83601214aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyWindow(WINDOW_NAME)\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyWindow('Original image')\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4efd05c-ab5b-4b5f-8c20-26ab03b16841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatGPT code to expand a mask\n",
    "\n",
    "# Load the image mask (assuming binary mask with clusters)\n",
    "image_mask = cv2.imread('image_mask.png', 0)  # Read the mask as grayscale (0 = grayscale mode)\n",
    "\n",
    "# Define the structuring element for dilation\n",
    "kernel = np.ones((3, 3), np.uint8)  # Adjust the kernel size based on desired expansion\n",
    "\n",
    "# Perform dilation to increase cluster size\n",
    "expanded_mask = cv2.dilate(image_mask, kernel, iterations=1)\n",
    "\n",
    "# Save the expanded mask\n",
    "cv2.imwrite('expanded_mask.png', expanded_mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}